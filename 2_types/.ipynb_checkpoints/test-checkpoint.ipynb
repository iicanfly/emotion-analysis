{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(self, filename):\n",
    "        \"\"\"Save the neural network to the file ``filename``.\"\"\"\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases],\n",
    "                \"cost\": self.cost.__name__.decode('utf-8')}\n",
    "        f = io.open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return np.sum(np.nan_to_num(-y * np.log(a) - (1 - y) * np.log(1 - a)))\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a - y)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import json\n",
    "#np.set_printoptions(threshold='nan')\n",
    "sizes = [23151,64,64,64,1]\n",
    "cost = CrossEntropyCost\n",
    "weights = [np.random.randn(y, x) / np.sqrt(x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "data[\"sizes\"] =sizes\n",
    "data[\"weights\"] = [w.tolist() for w in weights]\n",
    "data[\"biases\"] =  [b.tolist() for b in biases]\n",
    "data[\"cost\"] = cost.__name__\n",
    "\n",
    "\n",
    "f = io.open(\"./test_sizes.txt\", \"wb\")\n",
    "json.dump(data[\"sizes\"],f)\n",
    "f.close()\n",
    "f = io.open(\"./test_weights.txt\", \"wb\")\n",
    "json.dump(data[\"weights\"],f)\n",
    "f.close()\n",
    "f = io.open(\"./test_biases.txt\", \"wb\")\n",
    "json.dump(data[\"biases\"],f)\n",
    "f.close()\n",
    "f = io.open(\"./test_cost.txt\", \"wb\")\n",
    "json.dump(data[\"cost\"],f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([23151, 64, 64, 64], [64, 64, 64, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes[:-1], sizes[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[\"weights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[\"biases\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data['weights'][0]= 64\n",
      "data['weights'][1]= 64\n",
      "data['weights'][2]= 64\n",
      "data['weights'][3]= 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print \"data['weights'][%s]=\"%i , len(data[\"weights\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data['biases'][0]= 64\n",
      "data['biases'][1]= 64\n",
      "data['biases'][2]= 64\n",
      "data['biases'][3]= 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print \"data['biases'][%s]=\"%i , len(data[\"biases\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23151 64\n",
      "64 64\n",
      "64 64\n",
      "64 1\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(sizes[:-1], sizes[1:]):\n",
    "    print x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print data['weights'][0].shape\n",
    "# print data['weights'][1].shape\n",
    "# print data['weights'][2].shape\n",
    "# print data['weights'][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print data['biases'][0].shape\n",
    "# print data['biases'][1].shape\n",
    "# print data['biases'][2].shape\n",
    "# print data['biases'][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib={}\n",
    "lib[\"aa\"] = 1;\n",
    "lib[\"bb\"] = 0;\n",
    "lib[\"cc\"] = 0;\n",
    "lib[\"dd\"] = 0;\n",
    "lib[\"ee\"] = 1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aa': 1, 'bb': 0, 'cc': 0, 'dd': 0, 'ee': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[1, 0, 0, 1, 0]]), matrix([[1]]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import *\n",
    "#(mat(lib.values()).transpose(),mat([1]))\n",
    "(mat(lib.values()),mat([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat(lib.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat(lib.values()).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.matrixlib.defmatrix.matrix"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mat(lib.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load(filename_sizes, filename_weights, filename_biases, filename_cost):\n",
    "#     \"\"\"Load a neural network from the file ``filename``.  Returns an\n",
    "#     instance of Network.\n",
    "#     \"\"\"\n",
    "# filename_weights = \"./parameter/epoch0_weights.txt\"\n",
    "filename_weights = \"./test_weights.txt\"\n",
    "filename_sizes = './test_sizes.txt'\n",
    "filename_biases = './test_biases.txt'\n",
    "filename_cost = './parameter/epoch0_cost.txt'\n",
    "\n",
    "f = io.open(filename_weights, \"r\")\n",
    "data_weights = json.load(f)\n",
    "f.close()\n",
    "\n",
    "f = io.open(filename_sizes, \"r\", encoding = 'UTF-8')\n",
    "data_sizes = json.load(f)\n",
    "f.close()\n",
    "\n",
    "f = io.open(filename_biases, \"r\")\n",
    "data_biases = json.load(f)\n",
    "f.close()\n",
    "\n",
    "f = io.open(filename_cost, \"r\")\n",
    "data_cost = json.load(f)\n",
    "f.close()\n",
    "    \n",
    "#     cost = getattr(sys.modules[__name__], data_cost[\"cost\"])\n",
    "#     net = Network(data_sizes[\"sizes\"], cost = cost)\n",
    "#     net.weights = [np.array(w) for w in data_weights[\"weights\"]]\n",
    "#     net.biases = [np.array(b) for b in data_biases[\"biases\"]]\n",
    "#     return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights = [np.array(w) for w in data_weights[\"weights\"]]\n",
    "#data_weights\n",
    "type(data_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NeuralNetwork import Network\n",
    "cost = getattr(sys.modules[__name__], data_cost)\n",
    "net = Network(data_sizes, cost = cost)\n",
    "net.weights = [np.array(w) for w in data_weights]\n",
    "net.biases = [np.array(b) for b in data_biases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.CrossEntropyCost"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "for i in range(23151):\n",
    "    a.append([0])\n",
    "a[0] = [1]\n",
    "a[1] = [1]\n",
    "a[2] = [1]\n",
    "a[3] = [1]\n",
    "a[4] = [1]\n",
    "a[5] = [1]\n",
    "inputData = mat(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "print net.feedforward(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import json\n",
    "import math\n",
    "reload(sys) \n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "def buildWordLib(fileName):\n",
    "    wordSet = dict()\n",
    "    ifstream = io.open(fileName, 'r', encoding = 'UTF-8')\n",
    "    for line in ifstream:\n",
    "        line = line.strip('\\n')\n",
    "        line = line.split()\n",
    "        for word in line:\n",
    "            wordSet[word] = np.array([0,0])\n",
    "    ifstream.close()\n",
    "    return wordSet\n",
    "\n",
    "def tf_idf(infile, outfile,num):\n",
    "    wordSet = buildWordLib(infile)\n",
    "#     for x in wordSet:\n",
    "#         print type(x)\n",
    "    ifstream = io.open(infile,'r',encoding = 'utf-8')\n",
    "    cont = ifstream.read()\n",
    "    ifstream.read()\n",
    "    cont = cont.split('\\n')\n",
    "    data = {}\n",
    "    for i in range(len(cont)):\n",
    "        cont[i] = cont[i].split(' ')\n",
    "#     print wordSet\n",
    "#     print len(cont)\n",
    "#     print cont \n",
    "    count_a = 0\n",
    "    count_b = 0\n",
    "    count_c = 0\n",
    "#     for x in wordSet:\n",
    "#         count_a = count_a + 1\n",
    "#         if count_a % 1000 == 0:\n",
    "#             print count_a\n",
    "#         for i in range(len(cont)):\n",
    "#             if x in cont[i]:\n",
    "#                 wordSet[x] = wordSet[x] + np.array([1,0])\n",
    "#             for j in range(len(cont[i])):\n",
    "#                 if x == cont[i][j]:\n",
    "#                     wordSet[x] = wordSet[x] + np.array([0,1])\n",
    "\n",
    "    for i in range(len(cont)):\n",
    "        count_a = count_a + 1\n",
    "        if count_a % 1000 == 0:\n",
    "            print \"count_a\",count_a\n",
    "        flag = dict()\n",
    "#         print cont[0]\n",
    "        for x in cont[i]:\n",
    "#             print (x)\n",
    "            if x == u'':\n",
    "                break\n",
    "            if x in flag:\n",
    "                wordSet[x] = wordSet[x] + np.array([0,1])\n",
    "            else:\n",
    "                flag[x] = 1\n",
    "                wordSet[x] = wordSet[x] + np.array([0,1])\n",
    "                wordSet[x] = wordSet[x] + np.array([1,0])\n",
    "            \n",
    "#     print wordSet\n",
    "    for x in wordSet:\n",
    "        data[x] = 0.0\n",
    "    total = len(cont)\n",
    "    for x in wordSet:\n",
    "        data[x] = float(wordSet[x][1]) * (math.log((total + 1.0)/(float(wordSet[x][0]) + 1.0)) + 1.0)\n",
    "    data_last = {}\n",
    "    for i in range(num):\n",
    "        count_b = count_b + 1\n",
    "        if count_b % 1000 == 0:\n",
    "            print \"b\",count_b\n",
    "        max_data = 0\n",
    "        max_word = u''\n",
    "        for x in data :\n",
    "            if x not in data_last and data[x] > max_data:\n",
    "                max_data = data[x]\n",
    "                max_word = x\n",
    "        data_last[max_word] = max_data\n",
    "        \n",
    "    ofstream = io.open(outfile,'w',encoding = 'utf-8')\n",
    "    for i in range(len(cont)):\n",
    "        count_c = count_c + 1\n",
    "        if count_c % 1000 == 0:\n",
    "            print \"c\",count_c\n",
    "        string = ''\n",
    "        for each in cont[i]:\n",
    "            if each in data_last:\n",
    "                string += each + ' '\n",
    "        if i < len(cont) - 1:\n",
    "            string += '\\n'\n",
    "        ofstream.write(string.decode('utf-8'))\n",
    "    ofstream.close()\n",
    "        \n",
    "        \n",
    "    \n",
    "#     for x in data_last:\n",
    "#         print x,data_last[x] \n",
    "#     for x in data:\n",
    "#         print x, data[x]\n",
    "#     print data\n",
    "        \n",
    "tf_idf('./2/train_data.txt','./2/train_data_last.txt',23151)\n",
    "# tf_idf('./text_test.txt','./text_test_last',23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5%2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
